# Example Pipeline Outputs

This document describes the outputs generated by the African Football Match Prediction pipeline.

## Directory Structure After Running

```
R-frica_Predicts/
├── data/
│   ├── raw/                              # Downloaded Kaggle datasets
│   │   ├── [african_football_data].csv
│   │   └── [fifa_ranking_data].csv
│   └── processed/                        # Processed feature data
│       ├── matches_features.csv          # Feature-engineered dataset
│       └── test_data.rds                 # Test set for evaluation
├── models/                               # Trained model objects
│   ├── logistic_model.rds                # Multinomial logistic regression
│   ├── naivebayes_model.rds              # Naive Bayes classifier
│   ├── rf_model.rds                      # Random Forest (ranger)
│   └── rpart_model.rds                   # Decision Tree
├── outputs/                              # Performance metrics and results
│   ├── summary_model_performance.csv     # Overall accuracy and AUC by model
│   ├── cv_accuracy_summary.csv           # Cross-validation results
│   ├── [BestModel]_class_metrics.csv     # Per-class metrics for best model
│   ├── variable_importance_rf.csv        # Feature importance scores
│   └── test_predictions.csv              # Predictions on test set
└── plots/                                # Visualizations
    ├── confusion_matrix_logistic.png
    ├── confusion_matrix_naivebayes.png
    ├── confusion_matrix_randomforest.png
    ├── confusion_matrix_decisiontree.png
    ├── model_comparison.png              # Accuracy and AUC comparison
    ├── roc_curves_logistic.png
    ├── roc_curves_naivebayes.png
    ├── roc_curves_randomforest.png
    ├── roc_curves_decisiontree.png
    ├── variable_importance_rf.png        # Top feature importance
    └── decision_tree.png                 # Decision tree visualization
```

## Key Output Files

### 1. summary_model_performance.csv

Overall performance summary for all models:

| Model | Accuracy | AUC_home | AUC_draw | AUC_away | AUC_mean |
|-------|----------|----------|----------|----------|----------|
| RandomForest | 0.62 | 0.72 | 0.61 | 0.69 | 0.67 |
| Logistic | 0.59 | 0.68 | 0.58 | 0.65 | 0.64 |
| NaiveBayes | 0.56 | 0.65 | 0.55 | 0.62 | 0.61 |
| DecisionTree | 0.55 | 0.63 | 0.54 | 0.60 | 0.59 |

*(Example values - actual results depend on data)*

### 2. matches_features.csv

Feature-engineered dataset with columns:
- `match_date`: Date of the match
- `home_team`, `away_team`: Team names
- `home_score`, `away_score`: Match scores
- `team_rank_home`, `team_rank_away`: FIFA rankings
- `rank_difference`: Rank differential (home - away)
- `days_since_home`, `days_since_away`: Days since last match
- `home_goals_avg`, `away_goals_avg`: Rolling average goals scored
- `home_conceded_avg`, `away_conceded_avg`: Rolling average goals conceded
- `home_wins_last5`, `home_draws_last5`, `home_losses_last5`: Recent form
- `away_wins_last5`, `away_draws_last5`, `away_losses_last5`: Recent form
- `home_form`, `away_form`, `form_difference`: Form scores
- `home_advantage`: Home field indicator (always 1)
- `match_importance`: Tournament importance level
- `result`: Target variable (home/draw/away)

### 3. Visualizations

**Confusion Matrices**: Show predicted vs actual outcomes for each model
- Rows: Predicted class
- Columns: Actual class
- Color intensity: Frequency of predictions

**Model Comparison Chart**: Bar chart comparing accuracy and mean AUC across models

**ROC Curves**: One-vs-rest ROC curves for each class (home/draw/away)
- Shows model's ability to distinguish each outcome
- AUC scores indicate classification performance

**Variable Importance**: Top 15 most important features for Random Forest
- Higher values = more important for predictions
- Typically: rank_difference, form_difference, recent averages

**Decision Tree**: Visual representation of decision rules
- Interpretable tree showing how decisions are made
- Each node shows split criteria and class distribution

## Performance Metrics Explained

### Overall Accuracy
Percentage of correct predictions across all classes.

### AUC (Area Under Curve)
For multiclass problems, calculated using one-vs-rest approach:
- **AUC_home**: How well the model distinguishes home wins
- **AUC_draw**: How well the model distinguishes draws
- **AUC_away**: How well the model distinguishes away wins
- **AUC_mean**: Average AUC across all classes

Values range from 0.5 (random) to 1.0 (perfect).

### Per-Class Metrics (in class_metrics.csv)
- **Sensitivity (Recall)**: Percentage of actual cases correctly predicted
- **Specificity**: Percentage of non-cases correctly identified
- **Precision (Pos Pred Value)**: Percentage of positive predictions that are correct
- **F1**: Harmonic mean of precision and recall
- **Balanced Accuracy**: Average of sensitivity and specificity

## Expected Performance

Based on similar football prediction tasks:
- **Accuracy**: 50-65% (varies by dataset quality)
- **AUC**: 0.60-0.75 (multiclass is challenging)
- **Home wins** are typically easier to predict than draws
- **Draws** are the most difficult class to predict

Performance depends on:
- Data quality and completeness
- Number of matches in dataset
- Feature engineering quality
- Model hyperparameter tuning

## Using the Models for Predictions

To use trained models for new predictions:

```r
# Load a trained model
model <- readRDS("models/rf_model.rds")

# Prepare new data with same features
new_data <- data.frame(
  team_rank_home = 15,
  team_rank_away = 25,
  rank_difference = -10,
  home_goals_avg = 1.8,
  away_goals_avg = 1.4,
  # ... other features
)

# Make predictions
prediction <- predict(model, newdata = new_data)
probabilities <- predict(model, newdata = new_data, type = "prob")

print(prediction)      # Predicted class
print(probabilities)   # Class probabilities
```

## Interpreting Results

### What makes a good prediction model?
1. **Accuracy > 55%** (better than random guessing for 3 classes)
2. **Balanced performance** across all three classes
3. **High AUC** (> 0.65) indicating good class separation
4. **Interpretable features** that make football sense

### Key Predictive Features
Based on domain knowledge, expect these to be important:
1. FIFA rank difference
2. Recent form (wins/draws/losses)
3. Goals scored/conceded averages
4. Home advantage
5. Days since last match

### Model Selection
- **Random Forest**: Usually highest accuracy, good for predictions
- **Logistic Regression**: Interpretable probabilities, good baseline
- **Decision Tree**: Most interpretable, good for understanding patterns
- **Naive Bayes**: Fast, good when features are conditionally independent

## Next Steps

After reviewing outputs:
1. **Check model performance** in `summary_model_performance.csv`
2. **Examine confusion matrices** to understand error patterns
3. **Review variable importance** to validate feature engineering
4. **Analyze ROC curves** to assess class-wise performance
5. **Use best model** for future predictions or tournament forecasting

For questions or issues, please refer to the README.md or open an issue.
